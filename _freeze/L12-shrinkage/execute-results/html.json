{
  "hash": "4865cf37e6c2d897bcf942b6ab689651",
  "result": {
    "engine": "knitr",
    "markdown": "# Lecture 12: Shrinkage\n\n## Recap\n\nSo far, we have studied the following concepts:\n\n\n```{=tex}\n\\begin{itemize}\n\\item Metrics for assessing whether a biomarker is potentially useful\n\\item Discovery Rates for biomarker lists obtained by selecting a subset based on these metrics\n\\item Regression to the mean of performance metrics\n\\end{itemize}\n```\n\n## Multilevel Models\n\n### Motivation\n\nIn this chapter we are going to start thinking about what about multi-level models. These specify a joint statistical model, not just for an individual biomarker in an individual study, but either multiple biomarkers or multiple studies at the same time. And if we get adventurous before the end of the course we will write models where encompassing both multiple biomarkers and multiple studies. The idea for this chapter is to do one of these at the time.\n\nSome of the main concepts will be:\n\n\n```{=tex}\n\\begin{itemize}\n\\item Heterogeneity\n\\item Random Effects\n\\item Multi-level Models\n\\end{itemize}\n```\n\nThe methods we will utilize will be Monte Carlo Markov Chains (MCMC).\n\nAnalyses of individual biomarkers are seemingly separate problems, which in reality share important statistical features. Importantly they share common sources of noise from the technology.\n\nAnalyses of the same gene in different studies are seemingly identical problems, which in reality present important differences. Importantly they inherit differences in study inclusion criteria, design and populations.\n\n\\newpage\n\n### Data Structure\n\nLet's begin by reminding ourselves of the data structure that we're going to be using. It is essentially the data structure of our curated ovarian data, where you have a set of studies, each of which includes a matrix of biomarkers and a vector of labels.\n\n\n```{=html}\n<!---\n\\begin{figure}[h]\n\\centerline{\\includegraphics[width=.8\\textwidth]{figures/multilevel.pdf}}\n\\caption{\\label{podlaha2}}\n\\end{figure}\n\\newpage\n-->\n```\n\nLet's focus on a single biomarker, say CXCL12, in a single study. There you have a row of $X$ that represents the levels of CXCL12 and you have a role a corresponding row of labels. That's the structure on which we have looked at all our statistics so far, when we did when we did discovery. We did a battery of these, but we always did them one at a time.\n\nHere we are going to think about extending this paradigm in two directions. One is to look at variability across biomarkers within a study (this lecture), and the other is to look at variability of biomarker behavior across studies (next lecture).\n\n\\newpage\n\n## Multilevel Modeling of Genes\n\n### Reasons for modeling genes as if coming from a higher level population\n\n\n```{=tex}\n\\begin{itemize}\n\\item\nGenome features share sources of variation, both biological and technological.\nThey are not independent.  \n\\item\nMultilevel models consider the gene to gene variability explicitly.\n\\item\nSimultaneous estimation of many related quantities is an old\n  problem in statistics.\n\\item\nKeywords: Stein (50's), Empirical Bayes, Hierarchical Bayes.\n\\item\nMultilevel models have been explored for microarrays since the earliest\ndays. \n\\end{itemize}\n```\n\n\\newpage\n\nSince the \"MCMC revolution\" multilevel models have been the backbone of Bayesian data analysis\n\nIn genomic data sets they can be used to implement:\n\n\n```{=tex}\n\\begin{itemize} \n\\item Shrinkage\n\\item Sparsity / Built-in Hypothesis Testing\n\\item Modular Structures\n\\item Flexible \"not-so-parametric\" models via mixtures\n\\item Useful Latent Classes\n\\item Small Samples Uncertainty Assessment\n\\end{itemize}\n```\n\nThis is a useful introduction. [Laredo](http://research.iac.es/winterschool/2014/media/loredo/iac14-3-IntroMLMs.pdf) provides a very useful introduction (pages 1-28) and a cool application in astronomy. I recommend you go over pages 1-28 before going forward with our lecture notes.\n\nFor those who want to dig a bit deeper [Morris and Lysy](https://arxiv.org/pdf/1203.5610.pdf) offer a great review.\n\nMost textbooks on Bayesian statistics have chapters on multi-level models.\n\n\\newpage\n\n### Differential Expression Analysis of TCGA\n\nGene-level Summaries:\n\n\n```{=tex}\n\\begin{center} \n\\begin{tabular}{ll}\n\\hline\n Overall Average (Abundance) & $a_g$ \\\\ \\hline\n Difference Between Group Averages & $d_g$ \\\\ \\hline\n Within-class Standard deviation & $s_g$ \\\\ \\hline\n Their ratio & $d_g / s_g$ \\\\ \\hline\n\\end{tabular}\n\\end{center}\n```\n\ncontinuous genomic feature: gene expression microarray readout in the TCGA study\n\nbinary phenotype (optimal surgical debulking)\n\n\\newpage\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nlibrary(curatedOvarianData)\ndata(\"TCGA_eset\")\nXX = as.matrix(cbind(exprs(TCGA_eset)))\nYY = 1 * as.vector(pData(TCGA_eset)[,\"debulking\"]==\"optimal\")\nXX = XX[,!is.na(YY)];\nYY = YY[!is.na(YY)]\n```\n:::\n\n\n\\newpage\n\nCompute Gene-Level Summaries\n\n<!-- redo with class-conditional variance -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCompSummaries = function(XX,YY){\nNGenes = nrow(XX)\nSSS = data.frame(matrix(NA,NGenes,4))\ncolnames(SSS) = \n  c(\"abundance\",\"difference\",\"variance\",\"SNratio\")\nfor (gg in 1:NGenes){\n  SSS[gg,\"abundance\"] = mean(XX[gg,])\n  m1 = mean( XX[gg,YY==1] ); m0 = mean( XX[gg,YY==0] ); \n  n1 = sum(YY==1); n0 = sum(YY==0); \n  SSS[gg,\"difference\"] = m1 - m0\n  SSS[gg,\"variance\"] = ( sum( ( XX[gg,YY==1] - m1 )^2 ) + sum( ( XX[gg,YY==0] - m0 )^2 ) ) / (n0+n1-2)\n  SSS[gg,\"SNratio\"] = \n    SSS[gg,\"difference\"] / sqrt( SSS[gg,\"variance\"] )\n}\nreturn(SSS)\n}\nGeneSummaries = CompSummaries(XX,YY)\n```\n:::\n\n\n\\newpage\n\n### Exploring marginal distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(GeneSummaries[,\"abundance\"],nclass=100)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(GeneSummaries[,\"difference\"],nclass=100)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(GeneSummaries[,\"difference\"])\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(sqrt(GeneSummaries[,\"variance\"]),nclass=100)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(GeneSummaries[,\"SNratio\"],nclass=100)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n### Exploring joint distributions\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummaries[,\"abundance\"],GeneSummaries[,\"difference\"],pch=\".\")\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummaries[,\"abundance\"],sqrt(GeneSummaries[,\"variance\"]),pch=\".\")\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sqrt(GeneSummaries[,\"variance\"]),GeneSummaries[,\"difference\"],pch=\".\")\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummaries[,\"abundance\"],GeneSummaries[,\"SNratio\"],pch=\".\")\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n### Gene-specific Distribution\n\nWe begin with a relatively simple setting where the class-conditional distributions are gaussian.\n\n\n```{=tex}\n\\begin{tabular}{p{6in}} \n      \\begin{eqnarray*} \n        X_{0gi} | \\alpha_{g}, \\sigma_g^2 , \\delta_g &\\sim& \n        N \\left( \\alpha_{g} - \\pi \\delta_g, \\sigma_g^2 \\right)  \\\\\n        X_{1gi} | \\alpha_{g} , \\sigma_g^2, \\delta_g &\\sim& \n        N \\left( \\alpha_{g} + (1-\\pi) \\delta_g,   \\sigma_g^2 \\right)\n      \\end{eqnarray*}\n    \\end{tabular}\n```\n\nEach gene distribution is characterized by three parameters:\n\n$\\delta_g$: true mean difference across classes\n\n$\\sigma_g$: true noise, common to both classes\n\n$\\alpha_g$: true abundance (average overeall expression marginally)\n\nIn addition we have $\\pi$: proportion of label 1's, assumed known.\n\nGenes are assumed to be conditionally independent.\n\n::: callout-note\nIn this model, does the data on genes 2 through G give you information you did not have before about gene 1?\n\nWhy or why not?\n:::\n\n\\newpage\n\n### Distribution of gene specific parameters across the genome\n\nOu next step is to model the distribution of the gene specific parameters, to describe how these parameters vary across the genome. We have three of them and we can make all sorts of assumptions about how they are jointly distributed. Here are some options\n\n\n```{=tex}\n\\begin{tabular}{|p{4in}|p{4in}|}\n  \\hline\n  \\multicolumn{1}{|c|}{II. Independence}  &\n  \\multicolumn{1}{c|}{CI. S\\&N Independence} \\\\ \n  \\begin{eqnarray*}\n    \\alpha_g | \\tau^2  &\\sim&  N ( 0, \\tau^2 ) \\\\\n    \\delta_g | \\lambda^2 &\\sim& N(0,\\lambda^2) \\\\\n    \\sigma_g^{-2}| \\nu, \\beta &\\sim& Ga (\\nu, \\beta)\n  \\end{eqnarray*}\n  & \n  \\begin{eqnarray*}\n    \\alpha_g | \\tau^2, \\sigma^{2}_g  & \\sim & \n    N ( 0, {\\sigma^{2}_g} \\tau^2 ) \\\\\n    \\delta_g | \\lambda^2\n    & \\sim & N ( 0, \\lambda^2 ) \\\\\n    \\sigma^{-2}_g | \\nu, \\beta & \\sim & Ga (\\nu, \\beta) \n  \\end{eqnarray*} \\\\\n  \\hline\n  \\multicolumn{1}{|c|}{IC. A\\&N Independence}  &\n  \\multicolumn{1}{c|}{CC. Complete Conjugacy} \\\\\n  \\begin{eqnarray*}\n    \\alpha_g | \\tau^2 & \\sim & \n    N ( 0, \\tau^2 ) \\\\\n    \\delta _g | \\lambda^2, \\sigma^{2}_g  \n    & \\sim & N ( 0, {\\sigma^{2}_g} \\lambda^2 ) \\\\\n    \\sigma^{-2}_g | \\nu, \\beta & \\sim & Ga (\\nu, \\beta) \n  \\end{eqnarray*}& \n  \\begin{eqnarray*}\n    \\alpha_g | \\tau^2, \\sigma^{2}_g  & \\sim & \n    N ( 0, {\\sigma^{2}_g} \\tau^2 ) \\\\\n    \\delta _g | \\lambda^2, \\sigma^{2}_g  \n    & \\sim & N ( 0, {\\sigma^{2}_g} \\lambda^2 ) \\\\\n    \\sigma^{-2}_g | \\nu, \\beta & \\sim & Ga (\\nu, \\beta) \n  \\end{eqnarray*} \\\\\n  \\hline\n\\end{tabular}\n```\n\n\\newpage\n\n### Estimation Methods: 1. Empirical Bayes\n\nGene-level Parameters: $\\theta_g = (\\alpha_g,\\delta_g,\\sigma_g)$\n\nGenome-level Parameters: $\\gamma = \\tau, \\lambda, \\nu, \\beta$\n\nData: $D$\n\nLikelihood: $p( D | \\gamma, \\theta_1, \\ldots, \\theta_G)$\n\n\n```{=tex}\n\\begin{itemize} \n\\item Derive or approximate marginal Likelihood $p( D | \\gamma)$\n\\item Find $\\hat \\gamma$ maximizing marginal likelihood.\n\\item Estimate $\\theta_g$ one gene at the time using $p( D | \\hat \\gamma, \\theta_g)$ (other genes can be left out because of conditional independence)\n\\end{itemize}\n```\n\nThis bypasses specification of a prior at the top level, and still gives a shrinkage estimate of the gene-level parameters.\n\nFor example in the conjugate model written earlier:\n\n$$ E ( \\delta_g | D, \\gamma) = d_g  \\left( \\frac {1}{ 1 + \\frac {2}{\\lambda n}} \\right)$$\n\nThis provides a shrinkage estimate of $\\delta_g$. The amount of shrinkage is controlled by the estimate of $\\lambda$, the genome-wide variance of the $\\delta$'s\n\n\\newpage\n\n### Estimation Methods: 2. MCMC\n\nDraw samples of parameters to get a sense for their location and spread.\n\n\n```{=tex}\n\\begin{itemize} \n\\item Derive or approximate \"full conditional distributions\" \n\n$p( \\gamma | D, \\theta_1, \\ldots, \\theta_G))$\n\n$p( \\theta_1 | D, \\gamma, \\theta_2, \\ldots, \\theta_G))$\n\n....\n\n$p( \\theta_G | D, \\gamma, \\theta_1, \\ldots, \\theta_{G-1}))$\n\n\\item Iteratively simulate samples of parameters looping through these.\n\\item Summarize the simulation to derive parameter estimates.\n\n\\end{itemize}\n```\n\n\\newpage\n\n### MCMC Illustration\n\nWe select genes with an abundance of at least 5, and then take a random sample of the rest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhighAbundance = GeneSummaries[,\"abundance\"] > 5\nXXX = XX[highAbundance,]\nNha = sum(highAbundance)\nset.seed(117)\nNinclude = 900\ninclude = sample(1:Nha,Ninclude)\nXXX = XXX[include,]\nYYY = YY\nGeneSummInclude = CompSummaries(XXX,YYY)\n```\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(GeneSummInclude,pch=\".\")\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n### Conjugate Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiffexpModel =\"model\n{\n  precision.s ~ dnorm( 0.0, 0.01 ) T(0,);\n  precision.a ~ dnorm( 0.0, 0.01 ) T(0,);\n  precision.d ~ dnorm( 0.0, 0.01 ) T(0,);\n  for ( g in 1:G ){\n    sigma2inv[g] ~ dnorm( 0.0, precision.s ) T(0,);\n    alpha[g] ~ dnorm(cut, precision.a) T(cut,);\n    delta[g] ~ dnorm(0.0, sigma2inv[g] * precision.d);\n  }\n  for ( g in 1:G ) {\n    for ( i in 1:N ) {\n            XX[g,i] ~ dnorm( alpha[g] + \n            ( YY[i] * (1-pi) - \n            (1-YY[i]) * pi ) * delta[g], sigma2inv[g]);\n      } \n    }\n}\n\"\n```\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rjags)\nlibrary(R2jags)\nlibrary(coda)\ndiffexp = jags.model(textConnection(diffexpModel),\n                   data = list( XX = XXX, YY=YYY, N=length(YYY), G=nrow(XXX), pi=mean(YYY),cut=5),\n                   n.chains = 1,\n                   n.adapt = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 456300\n   Unobserved stochastic nodes: 2703\n   Total graph size: 464026\n\nInitializing model\n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc.out = coda.samples(diffexp,c(\"alpha\",\"delta\",\"sigma2inv\",\n                                  \"precision.a\",\"precision.d\",\"precision.s\"),\n                        n.iter = 1000,thin=10)\nalpha = mcmc.out[[1]][,grep(\"alpha\",colnames(mcmc.out[[1]]))]\ndelta = mcmc.out[[1]][,grep(\"delta\",colnames(mcmc.out[[1]]))]\nsigma = 1 / sqrt( mcmc.out[[1]][,grep(\"sigma2inv\",colnames(mcmc.out[[1]]))] )\nalpha.hat = apply(alpha,2,mean)\ndelta.hat = apply(delta,2,mean)\n```\n:::\n\n\n\\newpage\n\nOne way to look at shrinkage is to compare the posterior estimates to the corresponding gene-specific MLEs.\n\nLet's look at the $\\alpha$'s\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummInclude[,\"abundance\"],alpha.hat)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\nLet's now look at the $\\delta$'s\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummInclude[,\"difference\"],delta.hat)\nabline(0,1)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n::: callout-note\nwhy is the shrinkage of the alpha's so much less pronounced than the shrinkage of the delta's?\n:::\n\n\\newpage\n\n### Point and slab Model\n\nTo contrast, this code implements a point and slab model which assumes that some unknown proportion of genes have a $\\delta$ that is superclose to zero.\n\n::: callout-note\nCan you tell how I did that?\n\nHow do we interpert the hh variable?\n\nWhy is hh a good letter in this case?\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiffexpModel.01 =\"model\n{\n  precision.s ~ dnorm( 0.0, 0.01 ) T(0,);\n  precision.a ~ dnorm( 0.0, 0.01 ) T(0,);\n  precision.d ~ dnorm( 0.0, 0.01 ) T(0,);\n  ph0 ~ dunif(0,1);\n  for ( g in 1:G ){\n    hh[g] ~ dbern(1-ph0);\n    sigma2inv[g] ~ dnorm( 0.0, precision.s ) T(0,);\n    alpha[g] ~ dnorm(cut, precision.a) T(cut,);\n     delta[g] ~ dnorm(0.0, (1-hh[g]) * 10000 + hh[g] * sigma2inv[g] * (precision.d)  );\n  }\n  for ( g in 1:G ) {\n    for ( i in 1:N ) {\n            XX[g,i] ~ dnorm( alpha[g] + \n            ( YY[i] * (1-pi) - \n            (1-YY[i]) * pi ) * delta[g], sigma2inv[g] );\n      } \n    }\n}\n\"\n```\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rjags)\nlibrary(R2jags)\nlibrary(coda)\ndiffexp = jags.model(textConnection(diffexpModel.01),\n                   data = list( XX = XXX, YY=YYY, N=length(YYY), G=nrow(XXX), pi=mean(YYY),cut=5 ),\n                   n.chains = 1,\n                   n.adapt = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 456300\n   Unobserved stochastic nodes: 3604\n   Total graph size: 467629\n\nInitializing model\n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc.out = coda.samples(diffexp,c(\"alpha\",\"delta\",\"sigma2inv\",\"hh\",\"ph0\",\n                                  \"precision.a\",\"precision.d\",\"precision.s\"),\n                        n.iter = 1000,thin=10)\nalpha = mcmc.out[[1]][,grep(\"alpha\",colnames(mcmc.out[[1]]))]\ndelta = mcmc.out[[1]][,grep(\"delta\",colnames(mcmc.out[[1]]))]\nsigma = 1 / sqrt( mcmc.out[[1]][,grep(\"sigma2inv\",colnames(mcmc.out[[1]]))] )\nhh = mcmc.out[[1]][,grep(\"hh\",colnames(mcmc.out[[1]]))]\nalpha.hat = apply(alpha,2,mean)\ndelta.hat = apply(delta,2,mean)\ndelta.med = apply(delta,2,median)\nhh.hat = apply(hh,2,mean)\n```\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mcmc.out[[1]][,c(\"ph0\",\"precision.a\",\"precision.d\",\"precision.s\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIterations = 110:1100\nThinning interval = 10 \nNumber of chains = 1 \nSample size per chain = 100 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean       SD  Naive SE Time-series SE\nph0          0.9585 0.029661 0.0029661      0.0172528\nprecision.a  0.1305 0.006263 0.0006263      0.0006263\nprecision.d 22.9849 8.180652 0.8180652      2.6154521\nprecision.s  0.1117 0.005536 0.0005536      0.0004727\n\n2. Quantiles for each variable:\n\n              2.5%     25%     50%     75%   97.5%\nph0         0.8967  0.9414  0.9625  0.9846  0.9969\nprecision.a 0.1179  0.1267  0.1299  0.1347  0.1417\nprecision.d 6.7714 17.3542 23.4045 28.4747 37.6601\nprecision.s 0.1019  0.1076  0.1119  0.1154  0.1216\n```\n\n\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummInclude[,\"abundance\"],alpha.hat,pch=\".\")\nabline(0,1)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummInclude[,\"difference\"],delta.hat,pch=\".\")\nabline(0,1)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(GeneSummInclude[,\"difference\"],delta.med,pch=\".\")\nabline(0,1)\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n::: callout-note\ncontrast this shrinkage pattern to the fully conjugate\n:::\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(cbind(GeneSummInclude,hh.hat),pch=\".\")\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(delta.hat,hh.hat,pch=\".\")\n```\n\n::: {.cell-output-display}\n![](L12-shrinkage_files/figure-html/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n::: callout-note\nthink about how to construct an estimate of FDR if all hou have is hh.hat\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}