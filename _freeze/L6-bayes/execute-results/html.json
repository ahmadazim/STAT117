{
<<<<<<< HEAD
  "hash": "1c3b41103c63b7c3c8eff000dfa5f7fa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: February 8, 2024\n---\n\n\n# Lecture 6: Bayesian Modeling\n\n## Bayesian Models\n\nSo far we focused on statistical summaries, but have paid little attention to uncertainty about them. When we say that the PPV at a level of -1 for ZNF487 expression is .2, how sure are we? When we look at the comparison between the PR curves or benefit curves of two biomarkers, how reliably can we say that one is above the other? We are going to approach these questions using Bayesian models. Both \"Bayesian\" and \"model\" are very loaded words. We could go w/out models at all and rely on resampling techniques like the bootstrap to generate uncertainty statements. Or we could specify models, but use non-Bayesian methods, such as maximum likelihood, for our uncertainty. There are pros and cons of each of these choices. Models are powerful and can amplify the value of your data if you get them approximately right (a nontrivial task). Bayes is powerful, general and simple in concept, but a bit high maintenance compared to its frequentist counterparts. There are cool model-free or model-robust Bayesian techniques, but we will only scratch the surface there.\n\nIf you are completely new to Bayesian learning, here is [a gentle introduction with a cute animation](https://willhipson.netlify.app/post/bayesian_intro/binomial_gold/) and here is [the Bayesrulesbook chapter on the Beta-Binomial case](https://www.bayesrulesbook.com/chapter-3.html). You will not find it hard to scout material at the right level for you online. \"Modeling in Medical Decision Making\" has relevant chapters. I posted a copy on the canvas files. A great reference is [\\@Hoff2009]() A fun book, though perhaps way more than you signed up for, is [@mcelreath2020]. [\\@sarma2020chi](http://dx.doi.org/10.1145/3313831.3376377) offers a brief review on how to think about, and specify, prior distributions, often a sticky points for Bayesian analysis.\n\nBayesian learning uses probability to represent both variation / randomness in the real world, and incomplete knowledge. As a result, probabilities provide a selef contained system that formally represents knowledge in the face of uncertainty and can be used to update knowledge when new information is introduced. In general, we typically wish to understand characteristics that govern a population, from which we normally have a subset of data representing it. Population quantities capturing these characteristics are usually represented in terms of a parameter $\\theta$. The subset of data is represented through variables $x,y$.\n\n\\newpage\n\n### Bayes Rule\n\nThe following equation represents the basis of Bayesian modeling.\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\nP(\\theta | x,y) &= \\frac{P(x,y | \\theta) P(\\theta)}{\\int_\\theta P(x,y | \\theta) P(\\theta)d\\theta} = \\frac{P(x,y | \\theta) P(\\theta)}{P(x,y)}\\propto P(x,y | \\theta) P(\\theta)\n\\end{split}\n\\end{equation}\n```\n\n\\vspace{1mm}\n\nLikelihood: $P(x,y | \\theta)$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item Generating model for data $x,y$\n\\item Depends on unknown parameters $\\theta$'s\n\\end{itemize}\n```\n\nPrior: $P(\\theta)$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item Degree of plausibility of $\\theta$'s\n\\item What is known prior to observing the data\n\\end{itemize}\n\\vspace{1mm}\n```\n\nPosterior: $P(\\theta | x,y)$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item Updated degree of plausibility of $\\theta$'s\n\\item What is known after observing the data\n\\end{itemize}\n```\n\n\\newpage\n\n## Inference on Sensitivity and Specificity\n\n### Likelihood Case Control Design\n\nWe now focus, to begin, on a case-control example to evaluate a dichotomous biomarker. In this example, we assume that we know whether patients have a disease or not, which we represent with $y=1$ for a disease case, and $y=0$ for a non-disease case. Furthermore, we know the biomarker level of each patient, which is represented as $x=1$ if they have, say, a high level of expression, and a $x=0$ for a low level of expression. The following $2 \\times 2$ table represents such a scenario.\n\n\n```{=tex}\n\\begin{table}[ht]\n\\Large\n\\begin{tabular}{lccc}\n& \\multicolumn{2}{c}{Biomarker} &\\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$x=1$} & \\multicolumn{1}{c|}{$x=0$} & \\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{\"high\"} & \\multicolumn{1}{c|}{\"low\"} & \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=0$ (No Disease)} & \\multicolumn{1}{c|}{$m_0$} & \\multicolumn{1}{c|}{$n_0-m_0$} & \\multicolumn{1}{c|}{$n_0$} \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=1$ (Disease)} & \\multicolumn{1}{c|}{$m_1$} & \\multicolumn{1}{c|}{$n_1-m_1$} & \\multicolumn{1}{c|}{$n_1$} \\\\ \\hline\n\\end{tabular}\n\\end{table}\n```\n\nHere $m_0$ and $m_1$ represent the number of patients with no disease and a high biomarker, and the number of patients with disease and a high biomarker, respectively. The total counts for the margins for $y$ ($n_0$ and $n_1$) are fixed.\n\nIn such a set-up, the parameters of interest would usually then be the \\textit{sensitivity}, denoted as\n\n$$\n\\beta = p(x=1\\mid y=1)\n$$\n\nand \\textit{specificity},\n\n$$\n\\alpha = p(x=0|y=0)\n$$\n\nThe likelihood for learning about sensitivity and specificity from the observed data would then be,\n\n$$\nL_{CC} = \\binom{n_1}{m_1} \\beta^{m_1}\\left(1-\\beta\\right)^{n_1-m_1}\\binom{n_0}{m_0}\\left(1-\\alpha\\right)^{m_0}\\alpha^{n_0-m_0}\n$$ \\newpage\n\n### Likelihood for Population-Based Design\n\nWe note that such a construct assumes ahead of time that the total number of disease and disease-free individuals are fixed. This may be too restrictive in some settings. Therefore, we may alternatively fix the \\textbf{total} sample size instead, so that $n_0$ and $n_1$ may vary as long as their sum is equal to a fixed $n = n_0 + n_1$. Such a model, which we refer to as \"population based\", would allow more flexibility and implies the following table.\n\n\n```{=tex}\n\\begin{table}[ht]\n\\Large\n\\begin{tabular}{lccc}\n& \\multicolumn{2}{c}{Biomarker} &\\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$x=1$} & \\multicolumn{1}{c|}{$x=0$} & \\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{\"high\"} & \\multicolumn{1}{c|}{\"low\"} & \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=0$ (Disease)} & \\multicolumn{1}{c|}{$m_0$} & \\multicolumn{1}{c|}{$n_0-m_0$} & \\multicolumn{1}{c|}{$n_0$} \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=1$ (No Disease)} & \\multicolumn{1}{c|}{$m_1$} & \\multicolumn{1}{c|}{$n_1-m_1$} & \\multicolumn{1}{c|}{$n_1$} \\\\ \\hline\n& \\multicolumn{1}{l}{} & \\multicolumn{1}{l|}{} & \\multicolumn{1}{l|}{$n$} \\\\\n\\cline{4-4}\n\\end{tabular}\n\\end{table}\n```\n\nThe parameters associated with this table include the sensitivity and prevalence, $\\alpha, \\beta$ from before, in addition to the \\textit{prevalence}, represented as,\n\n$$\n\\pi = p(y=1)\n$$\n\nThe table may be rewritten in terms of these three parameters as a table of proportions, given by\n\n\n```{=tex}\n\\begin{table}[ht]\n\\Large\n\\begin{tabular}{lccc}\n& \\multicolumn{2}{c}{Biomarker} &\\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$x=1$} & \\multicolumn{1}{c|}{$x=0$} & \\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{\"high\"} & \\multicolumn{1}{c|}{\"low\"} & \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=0$ (No Disease)} & \\multicolumn{1}{c|}{$(1-\\pi)(1-\\alpha)$} & \\multicolumn{1}{c|}{$(1-\\pi)\\alpha$} & \\multicolumn{1}{c|}{$1-\\pi$} \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=1$ (Disease)} & \\multicolumn{1}{c|}{$\\pi\\beta$} & \\multicolumn{1}{c|}{$\\pi(1-\\beta)$} & \\multicolumn{1}{c|}{$\\pi$} \\\\ \\hline\n& \\multicolumn{1}{l}{} & \\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$1$} \\\\\n\\cline{4-4}\n\\end{tabular}\n\\end{table}\n```\n\nThen the associated likelihood function can be represented in terms of the case control likelihood as,\n\n$$\nL_{PB} = \\binom{n_0+n_1}{n_1}\\pi^{n_1}\\left(1-\\pi\\right)^{n_0}\\cdot L_{CC}\n$$\n\nThis model allows for the observation of a different number of disease and no disease cases conditional on a fixed total sample size. The prevalence can then be estimated under such a model, which we note takes a binomial mass function form through the likelihood above.\n\n### Posterior Inference\n\nThe likelihood function we have specified under both models can then be incorporated into a Bayesian model. Specifically, we may conduct Bayesian inference on the parameters of interest. Under the case control example, one idea may be to specify a uniform prior on both the parameters for specificity and sensitivity, $\\alpha, \\beta$. The prior is given as,\n\n$$\nP(\\alpha,\\beta) = 1, \\ \\ \\text{for} \\ \\ \\alpha \\in [0,1], \\beta \\in [0,1]\n$$\n\nwhich when combined with the likelihood $L_{CC}$ yields the posterior of the form,\n\n$$\nP(\\alpha,\\beta|m_0,m_1) \\propto\n1 \\times \\binom{n_1}{m_1} \\beta^{m_1} ( 1-\\beta) ^{n_1-m_1} \n\\binom{n_0}{m_0} (1-\\alpha)^{m_0} \\alpha ^{n_0-m_0}\n$$ \\newpage The uniform prior places equal mass on the possible support for $\\alpha,\\beta$. One can recognize that the kernel of the posterior is consistent with that of a Beta distribution and therefore the normalizing constants can be easily derived. If instead we had prior information regarding $\\alpha,\\beta$, we may instead model them each as a Beta distribution. Such a prior will jointly be,\n\n$$\nP(\\alpha,\\beta) =\\frac{\\Gamma(b_0+b_1)}{\\Gamma(b_1)\\Gamma(b_0)}\\beta^{b_1-1}(1-\\beta)^{b_0-1}\\frac{\\Gamma(a_0+a_1)}{\\Gamma(a_1)\\Gamma(a_0)}(1-\\alpha)^{a_1-1}\\alpha^{a_0-1}\n$$\n\nwhich when combined with the likelihood $L_{CC}$ will result in the posterior form,\n\n$$\nP(\\alpha,\\beta|m_0,m_1) \\propto \\beta^{m_1+b_1-1} ( 1-\\beta) ^{n_1-m_1 + b_0-1}(1-\\alpha)^{m_0 + a_1-1} \\alpha ^{n_0-m_0+a_0-1}\n$$\n\nFor the population based set-up, where we only fixed the total number of sample cases, we may put Beta priors on each of the three parameters of interest, $\\alpha, \\beta, \\pi$. The prior will then be \\textbf{conjugate} with the form,\n\n\\begin{equation}\n\\begin{split}\nP(\\alpha,\\beta,\\pi) & = \\frac{\\Gamma(p_0+p_1)}{\\Gamma(p_1)\\Gamma(p_0)}\\pi^{p_1-1}(1-\\pi)^{p_0-1}\\\n\\times \\\\\n&  \\frac{\\Gamma(b_0+b_1)}{\\Gamma(b_1)\\Gamma(b_0)}\\beta^{b_1-1}(1-\\beta)^{b_0-1}\\frac{\\Gamma(a_0+a_1)}{\\Gamma(a_1)\\Gamma(a_0)}(1-\\alpha)^{a_1-1}\\alpha^{a_0-1}\n\\end{split}\n\\end{equation}\\\\end{equation}\\\\end{equation}\\\\end{equation}\\\\end{equation}\n\nwhich leads to a posterior of the form,\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\nP(\\alpha,\\beta, \\pi|m_0,m_1) & \\propto \\pi^{n_1+p_1-1} ( 1-\\pi) ^{n_0+p_0-1} \\\\\n& \\beta^{m_1+b_1-1} ( 1-\\beta) ^{n_1-m_1 + b_0-1}(1-\\alpha)^{m_0 + a_1-1} \\alpha ^{n_0-m_0+a_0-1}\n\\end{split}\n\\end{equation}\n```\n\n\\newpage\n\n### Prediction of Future Observations\n\nNow suppose that we wanted to predict a future outcome $y^*$ for a subject with a biomarker level of $x^*$. Then, the \\textit{prior predictive distribution} is given by,\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\nP(y^*|x^*) &= \\int_{\\theta} P(y^* | x^*, \\theta) P(\\theta)d\\theta \\\\\n= \\int_{\\theta} \\dfrac{P(y^*,x^*| \\theta)}{P(x^*| \\theta)} P(\\theta)d\\theta\n\\end{split}\n\\end{equation}\n```\n\nThe prior predictive may be thought of as the data marginalized over the prior distribution, or the predictied value of a new data point before observing the actual sample itself. Related to this is the \\textit{posterior predictive distribution}, which is interpreted as the predicted value of a new data point after observing the sample data. The posterior predictive is given by,\n\n$$\nP(y^*|x^*, y, x) =\\int_{\\theta} P(y^* | x^*, \\theta) P(\\theta|y, x)d\\theta\n$$\n\n## Likelihood Principle\n\nRelated to the above is the \\textbf{Likelihood Principle}.\n\n\n```{=tex}\n\\begin{itemize}\n\\item Statement: The strength of evidence favoring a parameter value $\\theta_a$ against $\\theta_b$ (and vice versa) is captured\n$\\frac{P(y|\\theta_a)}{P(y|\\theta_b)}$\n\\item Maintained in Bayesian framework but not in Frequentist\n\\item Only what was observed is relevant\n\\item If two likelihoods have the same kernel, their inference should be identical\n\\end{itemize}\n```\n\n\\newpage\n\nFor example, two experiments to assess a proportion $\\theta$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item A coin is flipped 12 times, and three of the tosses were heads $P(y|\\theta) = \\binom{12}{3}\\theta^3(1-\\theta)^9$\n\\item A coin is flipped until the third heads, and it took 12 tosses $P(y|\\theta) = \\binom{11}{2}\\theta^3(1-\\theta)^9$\n\\item The Likelihood Principle says inference on $\\theta$ should be the same in both cases\n\\end{itemize}\n```\n\nWe can illustrate the above principle using an example. Using the two experiments on the previous page, we would like to evaluate\n\n\n```{=tex}\n\\begin{itemize}\n\\item $H_0$: $\\theta \\geq 0.5$\n\\item $H_A$: $\\theta < 0.5$\n\\item The Likelihood Principle says results should be the same\n\\end{itemize}\n\\vspace{3mm}\n```\n\nDifferent sampling models may result in different p-values:\n\n\n```{=tex}\n\\begin{itemize}\n\\item Binomial p-value $$P(y \\leq 3 | \\theta = 0.5) = \\sum_{i=0}^3 \\binom{12}{i} \\theta^i(1-\\theta)^{12-i}$$\n\\item Negative-binomial p-value $$P(y \\leq 3 | \\theta = 0.5) = \\sum_{i=11}^\\infty \\binom{i}{2} \\theta^3(1-\\theta)^{12-i}$$\n\n\\end{itemize}\n```\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(3, 12, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07299805\n```\n\n\n:::\n\n```{.r .cell-code}\n1-pnbinom(8, 3, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03271484\n```\n\n\n:::\n:::\n\n\n[\\@lavine2020nams](https://doi.org/10.1090/noti2114) Has important insight about this if you are interested. His brief and enlightening paper begins by saying: \"A fundamental idea in statistics and data science is that statistical procedures are judged by criteria such as misclassification rates, p-values, or convergence that measure how the procedure performs when applied to many possible data sets. But such measures gloss over quantifying the evidence in a particular data set. We show that assessing a procedure and assessing evidence are distinct. The main distinction is that procedures are assessed unconditionally, i.e., by averaging over many data sets, while evidence must be assessed conditionally by considering only the data at hand.\"\n\n\\newpage\n\n## Monte Carlo Explorations of Posterior Distributions\n\nWe now re-examine the case control scenario with a uniform prior and how it can be implemented in R. We first assume that $m_0 = 1, n_0 = 10, m_1 = 7$ and $n_1 = 10$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 = 7\nn1 = 10\nx0 = 1\nn0 = 10\n```\n:::\n\n\nThen we conduct $100,000$ draws from the posterior distributions of $\\alpha, \\beta$ and $\\pi$, under a uniform prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMM = 100000 # monte carlo draws\na0 = b0 = a1 = b1 = p0 = p1 = 1 # prior hyperparameters (uniform)\nbeta = rbeta(MM,x1+b1,n1-x1+b0)\nalpha = rbeta(MM,n0-x0+b0,x0+b1)\n```\n:::\n\n\nIn this case the posterior distribution is available in closed form, so many of the quantities we are interested in are available analytically, or via well-worn and accurate numerical approximations such as the Incomplete Beta function. Even here it is far more straightforward to explore properties of the posteior distribution by generating a sample. For example, it is trivial to derive distributions of arbitrary functions of multiple parameters, which would otherwise normally require some gymnastic around tansformations of variables.\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nhist(beta,nclass=100,xlim=c(0,1), xlab = expression(beta))\nhist(alpha,nclass=100,xlim=c(0,1), xlab = expression(alpha))\n```\n\n::: {.cell-output-display}\n![Monte Carlo Approximations of the Posterior Distributions of Parameters Alpha and Beta](L6-bayes_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe above figures are the histograms approximating the posterior densities for each of the parameters of interest.\n\n\\newpage\n\nWe can also obtain summary statistics by directly looking at the mean and standard deviation of the obtained samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8331511\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1034034\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6657966\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1306234\n```\n\n\n:::\n:::\n\n\nWe may also examine the positive distribution of the positive predictive value. This illustrate how easy it is to derive posterior distribution of transformations of the variables. To begin, we fix the prevalence of the population at $0.25$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi = 0.25\npi.x = (pi*beta)/(pi*beta + (1-pi)*(1-alpha))\nhist(pi.x, nclass = 100, xlim = c(0,1))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of the Positive Predictive Value](L6-bayes_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe can then obtain a point estimate of the mean of the posterior predictive at a prevalence rate of $0.25$ and the proportion of samples above $0.5$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(pi.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.603507\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(pi.x > 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.72132\n```\n\n\n:::\n:::\n\n\nThe posterior predictive value can be analyzed further. Specifically, we can plot the joint distribution of the posterior predictive value against $\\alpha$ and $\\beta$, made using `ggplot2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2), pty = \"s\")\nsmoothScatter(alpha,pi.x)\nsmoothScatter(beta,pi.x)\n```\n\n::: {.cell-output-display}\n![Appoximate Joint Distribution of the Positive Predictive Value and Alpha or Beta](L6-bayes_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\nWe may also analyze the sensitivity to a conjugate prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 = 10; b1 = 40\nbeta.pre = rbeta(MM,b1,b0)\nbeta.post = rbeta(MM,x1+b1,n1-x1+b0) \nplot(density(beta.post),lwd=2,xlab=\"BETA\",main=\"\") \nlines(density(beta.pre),lwd=2,col=3)\n```\n\n::: {.cell-output-display}\n![Posterior Visualizations with hyperparameters b0 = 10, b1 = 40](L6-bayes_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 = 2; b1 = 2\nbeta.pre = rbeta(MM,b1,b0)\nbeta.post = rbeta(MM,x1+b1,n1-x1+b0) \nplot(density(beta.post),lwd=2,xlab=\"BETA\",main=\"\") \nlines(density(beta.pre),lwd=2,col=3)\n```\n\n::: {.cell-output-display}\n![Posterior Visualizations under b0 = 2, b1 = 2](L6-bayes_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\nWe will now focus on the population-based example, whereas previously we saw the case-control study.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 = 7\nn1 = 10\nx0 = 2\nn0 = 27\n```\n:::\n\n\nThen we conduct $100,000$ draws from the posterior distributions of $\\alpha, \\beta$ and $\\pi$, under a uniform prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMM = 100000 # monte carlo draws\na0 = b0 = a1 = b1 = p0 = p1 = 1 # prior hyperparameters (uniform) \nbeta = rbeta(MM,x1+b1,n1-x1+b0)\nalpha = rbeta(MM,n0-x0+b0,x0+b1)\npi = rbeta(MM,n1+p1,n0+p1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(alpha,nclass=100,xlim=c(0,1), xlab = expression(alpha))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of for Alpha, Beta, and Pi Parameters](L6-bayes_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(beta,nclass=100,xlim=c(0,1), xlab = expression(beta))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of for Alpha, Beta, and Pi Parameters](L6-bayes_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(pi,nclass=100,xlim=c(0,1), xlab = expression(pi))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of for Alpha, Beta, and Pi Parameters](L6-bayes_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n:::\n\n\n\\newpage\n\nWe can now analyze the distribution associated with the positive predictive value under an unknown prevalence rate, in the population based model. We can obtain summary statistics as before,\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi.x = (pi*beta)/(pi*beta+(1-pi)*(1-alpha))\nmean(pi.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.715938\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(pi.x>.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.93074\n```\n\n\n:::\n:::\n\n\nThe histogram is as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pi.x,nclass=100,xlim=c(0,1), xlab = expression(pi))\n```\n\n::: {.cell-output-display}\n![Distribution of the Posterior Predictive Value](L6-bayes_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWe can then look at the alternate visualization of the joint distribution under an unknown prevalence rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,3), pty = \"s\")\nsmoothScatter(alpha, pi.x, xlab = expression(alpha))\nsmoothScatter(beta, pi.x, xlab = expression(beta))\nsmoothScatter(pi, pi.x, xlab = expression(pi))\n```\n\n::: {.cell-output-display}\n![Alternative Joint Distribution of the Positive Predictive Value and Alpha/Beta/Pi](L6-bayes_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pty=\"s\")\nsmoothScatter(log(beta/(1-alpha)),pi.x)\n```\n\n::: {.cell-output-display}\n![Joint Density of PPV versus Log Ratio of Beta/(1-Alpha)](L6-bayes_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\n::: callout-note\n## Discussion\n\nConsider this scenario: a patient you know well is asking for advice on how to process the risk information they are gathering online.\n\n-   how do you describe to them in simple words what the PPV probability represents?\n\n-   they also found a Bayesian statement of uncertainty about the relevant PPV (say a posterior IQR). How do you describe the role of the prior in generating this IQR? What questions should they be asking about that prior?\n:::\n\n\\newpage\n\n## References\n\n::: {#refs}\n:::\n",
=======
  "hash": "0191ba25ddfb01f9dcd756734a0f4ecb",
  "result": {
    "engine": "knitr",
    "markdown": "\\newpage\n\n# Lecture 6 (February 9, 2023) Bayesian Modeling\n\n## Bayesian Models\n\nSo far we focused on statistical summaries, but have paid little attention to uncertainty about them. When we say that the PPV at a level of -1 for ZNF487 expression is .2, how sure are we? When we look at the comparison between the PR curves or benefit curves of two biomarkers, how reliably can we say that one is above the other? We are going to approach these questions using Bayesian models. Both \"Bayesian\" and \"model\" are very loaded words. We could go w/out models at all and rely on resampling techniques like the bootstrap to generate uncertainty statements. Or we could specify models, but use non-Bayesian methods, such as maximum likelihood, for our uncertainty. There are pros and cons of each of these choices. Models are powerful and can amplify the value of your data if you get them approximately right (a nontrivial task). Bayes is powerful, general and simple in concept, but a bit high maintenance compared to its frequentist counterparts. There are cool model-free or model-robust Bayesian techniques, but we will only scratch the surface there.\n\nIf you are completely new to Bayesian learning, here is [a gentle introduction with a cute animation](https://willhipson.netlify.app/post/bayesian_intro/binomial_gold/) and here is [the Bayesrulesbook chapter on the Beta-Binomial case](https://www.bayesrulesbook.com/chapter-3.html). You will not find it hard to scout material at the right level for you online. \"Modeling in Medical Decision Making\" has relevant chapters. I posted a copy on the canvas files. A great reference is [@Hoff2009]() A fun book, though perhaps way more than you signed up for, is [@mcelreath2020]. [@sarma2020chi](http://dx.doi.org/10.1145/3313831.3376377) offers a brief review on how to think about, and specify, prior distributions, often a sticky points for Bayesian analysis.\n\nBayesian learning uses probability to represent both variation / randomness in the real world, and incomplete knowledge. As a result, probabilities provide a selef contained system that formally represents knowledge in the face of uncertainty and can be used to update knowledge when new information is introduced. In general, we typically wish to understand characteristics that govern a population, from which we normally have a subset of data representing it. Population quantities capturing these characteristics are usually represented in terms of a parameter $\\theta$. The subset of data is represented through variables $x,y$.\n\n\\newpage\n\n### Bayes Rule\n\nThe following equation represents the basis of Bayesian modeling.\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\nP(\\theta | x,y) &= \\frac{P(x,y | \\theta) P(\\theta)}{\\int_\\theta P(x,y | \\theta) P(\\theta)d\\theta} = \\frac{P(x,y | \\theta) P(\\theta)}{P(x,y)}\\propto P(x,y | \\theta) P(\\theta)\n\\end{split}\n\\end{equation}\n```\n\n\\vspace{1mm}\n\nLikelihood: $P(x,y | \\theta)$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item Generating model for data $x,y$\n\\item Depends on unknown parameters $\\theta$'s\n\\end{itemize}\n```\n\nPrior: $P(\\theta)$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item Degree of plausibility of $\\theta$'s\n\\item What is known prior to observing the data\n\\end{itemize}\n\\vspace{1mm}\n```\n\nPosterior: $P(\\theta | x,y)$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item Updated degree of plausibility of $\\theta$'s\n\\item What is known after observing the data\n\\end{itemize}\n```\n\n\\newpage\n\n## Inference on Sensitivity and Specificity\n\n### Likelihood Case Control Design\n\nWe now focus, to begin, on a case-control example to evaluate a dichotomous biomarker. In this example, we assume that we know whether patients have a disease or not, which we represent with $y=1$ for a disease case, and $y=0$ for a non-disease case. Furthermore, we know the biomarker level of each patient, which is represented as $x=1$ if they have, say, a high level of expression, and a $x=0$ for a low level of expression. The following $2 \\times 2$ table represents such a scenario.\n\n\n```{=tex}\n\\begin{table}[ht]\n\\Large\n\\begin{tabular}{lccc}\n& \\multicolumn{2}{c}{Biomarker} &\\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$x=1$} & \\multicolumn{1}{c|}{$x=0$} & \\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{\"high\"} & \\multicolumn{1}{c|}{\"low\"} & \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=0$ (No Disease)} & \\multicolumn{1}{c|}{$m_0$} & \\multicolumn{1}{c|}{$n_0-m_0$} & \\multicolumn{1}{c|}{$n_0$} \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=1$ (Disease)} & \\multicolumn{1}{c|}{$m_1$} & \\multicolumn{1}{c|}{$n_1-m_1$} & \\multicolumn{1}{c|}{$n_1$} \\\\ \\hline\n\\end{tabular}\n\\end{table}\n```\n\nHere $m_0$ and $m_1$ represent the number of patients with no disease and a high biomarker, and the number of patients with disease and a high biomarker, respectively. The total counts for the margins for $y$ ($n_0$ and $n_1$) are fixed.\n\nIn such a set-up, the parameters of interest would usually then be the \\textit{sensitivity}, denoted as\n\n$$\n\\beta = p(x=1\\mid y=1)\n$$\n\nand \\textit{specificity},\n\n$$\n\\alpha = p(x=0|y=0)\n$$\n\nThe likelihood for learning about sensitivity and specificity from the observed data would then be,\n\n$$\nL_{CC} = \\binom{n_1}{m_1} \\beta^{m_1}\\left(1-\\beta\\right)^{n_1-m_1}\\binom{n_0}{m_0}\\left(1-\\alpha\\right)^{m_0}\\alpha^{n_0-m_0}\n$$ \\newpage\n\n### Likelihood for Population-Based Design\n\nWe note that such a construct assumes ahead of time that the total number of disease and disease-free individuals are fixed. This may be too restrictive in some settings. Therefore, we may alternatively fix the \\textbf{total} sample size instead, so that $n_0$ and $n_1$ may vary as long as their sum is equal to a fixed $n = n_0 + n_1$. Such a model, which we refer to as \"population based\", would allow more flexibility and implies the following table.\n\n\n```{=tex}\n\\begin{table}[ht]\n\\Large\n\\begin{tabular}{lccc}\n& \\multicolumn{2}{c}{Biomarker} &\\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$x=1$} & \\multicolumn{1}{c|}{$x=0$} & \\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{\"high\"} & \\multicolumn{1}{c|}{\"low\"} & \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=0$ (Disease)} & \\multicolumn{1}{c|}{$m_0$} & \\multicolumn{1}{c|}{$n_0-m_0$} & \\multicolumn{1}{c|}{$n_0$} \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=1$ (No Disease)} & \\multicolumn{1}{c|}{$m_1$} & \\multicolumn{1}{c|}{$n_1-m_1$} & \\multicolumn{1}{c|}{$n_1$} \\\\ \\hline\n& \\multicolumn{1}{l}{} & \\multicolumn{1}{l|}{} & \\multicolumn{1}{l|}{$n$} \\\\\n\\cline{4-4}\n\\end{tabular}\n\\end{table}\n```\n\nThe parameters associated with this table include the sensitivity and prevalence, $\\alpha, \\beta$ from before, in addition to the \\textit{prevalence}, represented as,\n\n$$\n\\pi = p(y=1)\n$$\n\nThe table may be rewritten in terms of these three parameters as a table of proportions, given by\n\n\n```{=tex}\n\\begin{table}[ht]\n\\Large\n\\begin{tabular}{lccc}\n& \\multicolumn{2}{c}{Biomarker} &\\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$x=1$} & \\multicolumn{1}{c|}{$x=0$} & \\\\ \n\\cline{2-3}\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{\"high\"} & \\multicolumn{1}{c|}{\"low\"} & \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=0$ (No Disease)} & \\multicolumn{1}{c|}{$(1-\\pi)(1-\\alpha)$} & \\multicolumn{1}{c|}{$(1-\\pi)\\alpha$} & \\multicolumn{1}{c|}{$1-\\pi$} \\\\ \n\\hline\n\\multicolumn{1}{|l|}{$y=1$ (Disease)} & \\multicolumn{1}{c|}{$\\pi\\beta$} & \\multicolumn{1}{c|}{$\\pi(1-\\beta)$} & \\multicolumn{1}{c|}{$\\pi$} \\\\ \\hline\n& \\multicolumn{1}{l}{} & \\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{$1$} \\\\\n\\cline{4-4}\n\\end{tabular}\n\\end{table}\n```\n\nThen the associated likelihood function can be represented in terms of the case control likelihood as,\n\n$$\nL_{PB} = \\binom{n_0+n_1}{n_1}\\pi^{n_1}\\left(1-\\pi\\right)^{n_0}\\cdot L_{CC}\n$$\n\nThis model allows for the observation of a different number of disease and no disease cases conditional on a fixed total sample size. The prevalence can then be estimated under such a model, which we note takes a binomial mass function form through the likelihood above.\n\n### Posterior Inference\n\nThe likelihood function we have specified under both models can then be incorporated into a Bayesian model. Specifically, we may conduct Bayesian inference on the parameters of interest. Under the case control example, one idea may be to specify a uniform prior on both the parameters for specificity and sensitivity, $\\alpha, \\beta$. The prior is given as,\n\n$$\nP(\\alpha,\\beta) = 1, \\ \\ \\text{for} \\ \\ \\alpha \\in [0,1], \\beta \\in [0,1]\n$$\n\nwhich when combined with the likelihood $L_{CC}$ yields the posterior of the form,\n\n$$\nP(\\alpha,\\beta|m_0,m_1) \\propto\n1 \\times \\binom{n_1}{m_1} \\beta^{m_1} ( 1-\\beta) ^{n_1-m_1} \n\\binom{n_0}{m_0} (1-\\alpha)^{m_0} \\alpha ^{n_0-m_0}\n$$ \\newpage The uniform prior places equal mass on the possible support for $\\alpha,\\beta$. One can recognize that the kernel of the posterior is consistent with that of a Beta distribution and therefore the normalizing constants can be easily derived. If instead we had prior information regarding $\\alpha,\\beta$, we may instead model them each as a Beta distribution. Such a prior will jointly be,\n\n$$\nP(\\alpha,\\beta) =\\frac{\\Gamma(b_0+b_1)}{\\Gamma(b_1)\\Gamma(b_0)}\\beta^{b_1-1}(1-\\beta)^{b_0-1}\\frac{\\Gamma(a_0+a_1)}{\\Gamma(a_1)\\Gamma(a_0)}(1-\\alpha)^{a_1-1}\\alpha^{a_0-1}\n$$\n\nwhich when combined with the likelihood $L_{CC}$ will result in the posterior form,\n\n$$\nP(\\alpha,\\beta|m_0,m_1) \\propto \\beta^{m_1+b_1-1} ( 1-\\beta) ^{n_1-m_1 + b_0-1}(1-\\alpha)^{m_0 + a_1-1} \\alpha ^{n_0-m_0+a_0-1}\n$$\n\nFor the population based set-up, where we only fixed the total number of sample cases, we may put Beta priors on each of the three parameters of interest, $\\alpha, \\beta, \\pi$. The prior will then be \\textbf{conjugate} with the form,\n\n\\begin{equation}\n\\begin{split}\nP(\\alpha,\\beta,\\pi) & = \\frac{\\Gamma(p_0+p_1)}{\\Gamma(p_1)\\Gamma(p_0)}\\pi^{p_1-1}(1-\\pi)^{p_0-1}\\\n\\times \\\\\n&  \\frac{\\Gamma(b_0+b_1)}{\\Gamma(b_1)\\Gamma(b_0)}\\beta^{b_1-1}(1-\\beta)^{b_0-1}\\frac{\\Gamma(a_0+a_1)}{\\Gamma(a_1)\\Gamma(a_0)}(1-\\alpha)^{a_1-1}\\alpha^{a_0-1}\n\\end{split}\n\\end{equation}\\\\end{equation}\\\\end{equation}\\\\end{equation}\n\nwhich leads to a posterior of the form,\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\nP(\\alpha,\\beta, \\pi|m_0,m_1) & \\propto \\pi^{n_1+p_1-1} ( 1-\\pi) ^{n_0+p_0-1} \\\\\n& \\beta^{m_1+b_1-1} ( 1-\\beta) ^{n_1-m_1 + b_0-1}(1-\\alpha)^{m_0 + a_1-1} \\alpha ^{n_0-m_0+a_0-1}\n\\end{split}\n\\end{equation}\n```\n\n\\newpage\n\n### Prediction of Future Observations\n\nNow suppose that we wanted to predict a future outcome $y^*$ for a subject with a biomarker level of $x^*$. Then, the \\textit{prior predictive distribution} is given by,\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\nP(y^*|x^*) &= \\int_{\\theta} P(y^* | x^*, \\theta) P(\\theta)d\\theta \\\\\n= \\int_{\\theta} \\dfrac{P(y^*,x^*| \\theta)}{P(x^*| \\theta)} P(\\theta)d\\theta\n\\end{split}\n\\end{equation}\n```\n\nThe prior predictive may be thought of as the data marginalized over the prior distribution, or the predictied value of a new data point before observing the actual sample itself. Related to this is the \\textit{posterior predictive distribution}, which is interpreted as the predicted value of a new data point after observing the sample data. The posterior predictive is given by,\n\n$$\nP(y^*|x^*, y, x) =\\int_{\\theta} P(y^* | x^*, \\theta) P(\\theta|y, x)d\\theta\n$$\n\n## Likelihood Principle\n\nRelated to the above is the \\textbf{Likelihood Principle}.\n\n\n```{=tex}\n\\begin{itemize}\n\\item Statement: The strength of evidence favoring a parameter value $\\theta_a$ against $\\theta_b$ (and vice versa) is captured\n$\\frac{P(y|\\theta_a)}{P(y|\\theta_b)}$\n\\item Maintained in Bayesian framework but not in Frequentist\n\\item Only what was observed is relevant\n\\item If two likelihoods have the same kernel, their inference should be identical\n\\end{itemize}\n```\n\n\\newpage\n\nFor example, two experiments to assess a proportion $\\theta$\n\n\n```{=tex}\n\\begin{itemize} \\small\n\\item A coin is flipped 12 times, and three of the tosses were heads $P(y|\\theta) = \\binom{12}{3}\\theta^3(1-\\theta)^9$\n\\item A coin is flipped until the third heads, and it took 12 tosses $P(y|\\theta) = \\binom{11}{2}\\theta^3(1-\\theta)^9$\n\\item The Likelihood Principle says inference on $\\theta$ should be the same in both cases\n\\end{itemize}\n```\n\nWe can illustrate the above principle using an example. Using the two experiments on the previous page, we would like to evaluate\n\n\n```{=tex}\n\\begin{itemize}\n\\item $H_0$: $\\theta \\geq 0.5$\n\\item $H_A$: $\\theta < 0.5$\n\\item The Likelihood Principle says results should be the same\n\\end{itemize}\n\\vspace{3mm}\n```\n\nDifferent sampling models may result in different p-values:\n\n\n```{=tex}\n\\begin{itemize}\n\\item Binomial p-value $$P(y \\leq 3 | \\theta = 0.5) = \\sum_{i=0}^3 \\binom{12}{i} \\theta^i(1-\\theta)^{12-i}$$\n\\item Negative-binomial p-value $$P(y \\leq 3 | \\theta = 0.5) = \\sum_{i=11}^\\infty \\binom{i}{2} \\theta^3(1-\\theta)^{12-i}$$\n\n\\end{itemize}\n```\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(3, 12, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07299805\n```\n\n\n:::\n\n```{.r .cell-code}\n1-pnbinom(8, 3, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03271484\n```\n\n\n:::\n:::\n\n\n[@lavine2020nams](https://doi.org/10.1090/noti2114) Has important insight about this if you are interested. His brief and enlightening paper begins by saying: \"A fundamental idea in statistics and data science is that statistical procedures are judged by criteria such as misclassification rates, p-values, or convergence that measure how the procedure performs when applied to many possible data sets. But such measures gloss over quantifying the evidence in a particular data set. We show that assessing a procedure and assessing evidence are distinct. The main distinction is that procedures are assessed unconditionally, i.e., by averaging over many data sets, while evidence must be assessed conditionally by considering only the data at hand.\"\n\n\\newpage\n\n## Monte Carlo Explorations of Posterior Distributions\n\nWe now re-examine the case control scenario with a uniform prior and how it can be implemented in R. We first assume that $m_0 = 1, n_0 = 10, m_1 = 7$ and $n_1 = 10$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 = 7\nn1 = 10\nx0 = 1\nn0 = 10\n```\n:::\n\n\nThen we conduct $100,000$ draws from the posterior distributions of $\\alpha, \\beta$ and $\\pi$, under a uniform prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMM = 100000 # monte carlo draws\na0 = b0 = a1 = b1 = p0 = p1 = 1 # prior hyperparameters (uniform)\nbeta = rbeta(MM,x1+b1,n1-x1+b0)\nalpha = rbeta(MM,n0-x0+b0,x0+b1)\n```\n:::\n\n\nIn this case the posterior distribution is available in closed form, so many of the quantities we are interested in are available analytically, or via well-worn and accurate numerical approximations such as the Incomplete Beta function. Even here it is far more straightforward to explore properties of the posteior distribution by generating a sample. For example, it is trivial to derive distributions of arbitrary functions of multiple parameters, which would otherwise normally require some gymnastic around tansformations of variables.\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nhist(beta,nclass=100,xlim=c(0,1), xlab = expression(beta))\nhist(alpha,nclass=100,xlim=c(0,1), xlab = expression(alpha))\n```\n\n::: {.cell-output-display}\n![Monte Carlo Approximations of the Posterior Distributions of Parameters Alpha and Beta](L6-bayes_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe above figures are the histograms approximating the posterior densities for each of the parameters of interest.\n\n\\newpage\n\nWe can also obtain summary statistics by directly looking at the mean and standard deviation of the obtained samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8333319\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1032169\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6667144\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1302649\n```\n\n\n:::\n:::\n\n\nWe may also examine the positive distribution of the positive predictive value. This illustrate how easy it is to derive posterior distribution of transformations of the variables. To begin, we fix the prevalence of the population at $0.25$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi = 0.25\npi.x = (pi*beta)/(pi*beta + (1-pi)*(1-alpha))\nhist(pi.x, nclass = 100, xlim = c(0,1))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of the Positive Predictive Value](L6-bayes_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe can then obtain a point estimate of the mean of the posterior predictive at a prevalence rate of $0.25$ and the proportion of samples above $0.5$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(pi.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.603947\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(pi.x > 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.72398\n```\n\n\n:::\n:::\n\n\nThe posterior predictive value can be analyzed further. Specifically, we can plot the joint distribution of the posterior predictive value against $\\alpha$ and $\\beta$, made using `ggplot2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2), pty = \"s\")\nsmoothScatter(alpha,pi.x)\nsmoothScatter(beta,pi.x)\n```\n\n::: {.cell-output-display}\n![Appoximate Joint Distribution of the Positive Predictive Value and Alpha or Beta](L6-bayes_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\nWe may also analyze the sensitivity to a conjugate prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 = 10; b1 = 40\nbeta.pre = rbeta(MM,b1,b0)\nbeta.post = rbeta(MM,x1+b1,n1-x1+b0) \nplot(density(beta.post),lwd=2,xlab=\"BETA\",main=\"\") \nlines(density(beta.pre),lwd=2,col=3)\n```\n\n::: {.cell-output-display}\n![Posterior Visualizations with hyperparameters b0 = 10, b1 = 40](L6-bayes_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 = 2; b1 = 2\nbeta.pre = rbeta(MM,b1,b0)\nbeta.post = rbeta(MM,x1+b1,n1-x1+b0) \nplot(density(beta.post),lwd=2,xlab=\"BETA\",main=\"\") \nlines(density(beta.pre),lwd=2,col=3)\n```\n\n::: {.cell-output-display}\n![Posterior Visualizations under b0 = 2, b1 = 2](L6-bayes_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\nWe will now focus on the population-based example, whereas previously we saw the case-control study.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 = 7\nn1 = 10\nx0 = 2\nn0 = 27\n```\n:::\n\n\nThen we conduct $100,000$ draws from the posterior distributions of $\\alpha, \\beta$ and $\\pi$, under a uniform prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMM = 100000 # monte carlo draws\na0 = b0 = a1 = b1 = p0 = p1 = 1 # prior hyperparameters (uniform) \nbeta = rbeta(MM,x1+b1,n1-x1+b0)\nalpha = rbeta(MM,n0-x0+b0,x0+b1)\npi = rbeta(MM,n1+p1,n0+p1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(alpha,nclass=100,xlim=c(0,1), xlab = expression(alpha))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of for Alpha, Beta, and Pi Parameters](L6-bayes_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(beta,nclass=100,xlim=c(0,1), xlab = expression(beta))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of for Alpha, Beta, and Pi Parameters](L6-bayes_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(pi,nclass=100,xlim=c(0,1), xlab = expression(pi))\n```\n\n::: {.cell-output-display}\n![Posterior Distribution of for Alpha, Beta, and Pi Parameters](L6-bayes_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n:::\n\n\n\\newpage\n\nWe can now analyze the distribution associated with the positive predictive value under an unknown prevalence rate, in the population based model. We can obtain summary statistics as before,\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi.x = (pi*beta)/(pi*beta+(1-pi)*(1-alpha))\nmean(pi.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7170034\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(pi.x>.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.93317\n```\n\n\n:::\n:::\n\n\nThe histogram is as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pi.x,nclass=100,xlim=c(0,1), xlab = expression(pi))\n```\n\n::: {.cell-output-display}\n![Distribution of the Posterior Predictive Value](L6-bayes_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWe can then look at the alternate visualization of the joint distribution under an unknown prevalence rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,3), pty = \"s\")\nsmoothScatter(alpha, pi.x, xlab = expression(alpha))\nsmoothScatter(beta, pi.x, xlab = expression(beta))\nsmoothScatter(pi, pi.x, xlab = expression(pi))\n```\n\n::: {.cell-output-display}\n![Alternative Joint Distribution of the Positive Predictive Value and Alpha/Beta/Pi](L6-bayes_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pty=\"s\")\nsmoothScatter(log(beta/(1-alpha)),pi.x)\n```\n\n::: {.cell-output-display}\n![Joint Density of PPV versus Log Ratio of Beta/(1-Alpha)](L6-bayes_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\n::: callout-note\n## Discussion\n\nConsider this scenario: a patient you know well is asking for advice on how to process the risk information they are gathering online.\n\n-   how do you describe to them in simple words what the PPV probability represents?\n\n-   they also found a Bayesian statement of uncertainty about the relevant PPV (say a posterior IQR). How do you describe the role of the prior in generating this IQR? What questions should they be asking about that prior?\n:::\n\n\\newpage\n\n## References\n\n::: {#refs}\n:::\n",
>>>>>>> 92c20afe76ab07d31ab0497a205dfd81dae5acec
    "supporting": [
      "L6-bayes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}