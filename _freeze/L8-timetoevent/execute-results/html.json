{
  "hash": "62589061758de6d648c21619e8d691ae",
  "result": {
    "engine": "knitr",
    "markdown": "\\newpage\n\n# Lecture 8 (February 16, 2023) Time to Event Data\n\n## Time-to-Event Biomarkers\n\nWe will now look at time-to-event outcomes. In many clinical applications, the researcher will have access to both whether or not an event occurred, but the also \\textit{when} the event occurred. Only considering one of the two pieces of information will result in an either incomplete or incorrect analysis of a problem of interest. Being able to use both types of data results in better inferences. \n\n### Censored Data\n\nAn important new concept in this lecture is that of censored data. \nFor example, the ovarian cancer studies considered in curatedOvarianData, inluding Yoshihara B, only followed patient for a limited period of time. As a result, the time elapsed between diagnosis and death (survival time) is known for some but not for all patients. The outcome data includes \"days_to_death\" and \"vital_status\". Let's take a look. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(curatedOvarianData)\ndata(GSE32063_eset)\nlibrary(survival)\nhead(cbind(pData(GSE32063_eset)[,\"days_to_death\"],\n           pData(GSE32063_eset)[,\"vital_status\"]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]   [,2]      \n[1,] \"780\"  \"living\"  \n[2,] \"1110\" \"living\"  \n[3,] \"600\"  \"deceased\"\n[4,] \"2910\" \"living\"  \n[5,] \"1710\" \"deceased\"\n[6,] \"810\"  \"deceased\"\n```\n\n\n:::\n:::\n\n\nThe vital status variable captures whether a patient was alive at the end of the study, in which case the days-to-death variable is a lower bound to the time . The jargon for this in biostat is that the time-to-the event is \"right censored\". Censored data will require different statistical approaches (e.g. they come with a different likelihood function) compared to non-censored data. Ignoring censoring can lead to substantial biases. Thinking hard about the censoring mechanism is also very important.\n\nThis is a commonly used way to encode the censoring information:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSurvObj = \n  Surv( time = pData(GSE32063_eset)[,\"days_to_death\"], \n        event = pData(GSE32063_eset)[,\"vital_status\"]==\"deceased\")\nhead(SurvObj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  780+ 1110+  600  2910+ 1710   810 \n```\n\n\n:::\n:::\n\n\n\\newpage\n\n### Survival and Hazard Functions\n\nA quick reminder of the terminology: the survival function is the probability of surviving until time $t$. The hazard $h$, or intensity, is the rate of occurrence of the event relative to the number of individual still available to experience it.\n\n\\begin{eqnarray*}\nS(t) & = & P ( \\mbox{Event occurs after time t}) \\\\\nF(t) & = & 1 - S(t) \\\\\ndF(t) / dt & = & f(t) \\\\\nh(t) & = & f(t) / S(t)\n\\end{eqnarray*}\n\nYou can go back and forth between $h$ and $F$ under smoothenss conditions.\n\nTo check you followed the logic so far, say $S(t)$ depends on some parameters $\\theta$ and try to write down the likelihood function for $\\theta$ the six observations in the code chunk just above.\n\n### Kaplan-Maier Construction\n\nThe Kaplan-Maier algorithm provides a nonparametric maximum likelihood estimate of the survival function for censored data. It is the counterpart of the empirical survival function (i.e.  one minus the empirical c.d.f.) which we may use in fully observed data.\n\nThe following plot yields the Kaplan-Meier curve. The survfit functions produces this if you set up a \"regression\" only including the intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survminer)\nkm.as.one = survfit (SurvObj ~ 1)\nggsurvplot(km.as.one,data=SurvObj,\n           risk.table.fontsize = 4, break.time.by = 365,\n           tables.theme = theme_cleantable(), \n           axes.offset = FALSE, tables.y.text = FALSE,\n           risk.table = \"nrisk_cumcensor\")\n```\n\n::: {.cell-output-display}\n![Kaplan-Meier Curve](L8-timetoevent_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe [Kaplan Meier learning tool](https://rubenvp.shinyapps.io/kmplotter/) and associated [Kaplan Meier curves: an introduction](https://rmvpaeme.github.io/KaplanMeier_intro/) tutorial are a good way to get a sense for how KM works, with code. Check out the \"worse and best case scenario\" figures in the tutorial. They give you a sense for the extra uncertainty brought in by censoring. This uncertainty is in addition to the sampling uncertainty we are all accustomed to. \nAnother [Introduction to Survival Analysis using R](https://shariq-mohammed.github.io/files/cbsa2019/1-intro-to-survival.html).\n\n\\newpage\nNext, this is the same population split by debulking status. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survminer)\nDebulking = pData(GSE32063_eset)[,\"debulking\"]\nkm.by.debulking = survfit (SurvObj ~ Debulking)\nggsurvplot(km.by.debulking,data=SurvObj,\n           risk.table = \"nrisk_cumcensor\",conf.int = TRUE,\n           risk.table.fontsize = 4, break.time.by = 365,\n           tables.theme = theme_cleantable(), \n           axes.offset = FALSE, tables.y.text = FALSE)\n```\n\n::: {.cell-output-display}\n![Kaplan-Meier Curve by Debulking Status](L8-timetoevent_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n\n## Modeling Dependence Between a Biomarker and a Censored Outcome\n\nWe normally visualize dependence via scatterplots. In this case the response variable is censored, so a scatterplot might look something like this. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npendant.plot = function(time,status,biomarker){\n  TT = max(time)*1.17\n  plot(biomarker[status==\"deceased\"],time[status==\"deceased\"],xlab=\"Biomarker\",ylab=\"Event Time\",pch=16,cex=2,xlim=c(min(biomarker),max(biomarker)),ylim=c(min(time),max(time)))\n  points(biomarker[status==\"living\"],time[status==\"living\"],pch=1,cex=2)\n  segments(biomarker[status==\"living\"],time[status==\"living\"],\n           biomarker[status==\"living\"],rep(TT,sum(status==\"living\")))\n}\npendant.plot(time=pData(GSE32063_eset)[,\"days_to_death\"],\n          status=pData(GSE32063_eset)[,\"vital_status\"],\n          biomarker=exprs(GSE32063_eset)[\"ZNF487\",])\n```\n\n::: {.cell-output-display}\n![Scatterplot of survival time by ZNF487 expression](L8-timetoevent_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\\newpage\n\n### Concordance Index\n\nThere is a popular way to compute a nonparametric measure of dependence that accounts for this type of censoring: the *concordance index*. The basic building block are concordant pairs: a pair of patients is called concordant if the biomarker is higher for the patient who experiences the event at a later timepoint. With censored data, we exclude all the pairs for which it is not possible to established whether they are concordant or not. The concordance probability (C-index) is the proportion of concordant pairs among all pairs of subjects. Similar to the AUC, 50 percent represents no dependence. A C-index above 50 percent indicates a negative dependence and a value below 50 percent indicates a positive dependence. For details see [What is Harrellâ€™s C-index?](https://statisticaloddsandends.wordpress.com/2019/10/26/what-is-harrells-c-index/)\n\nLet's look at ZNF487:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconcordance(SurvObj ~ exprs(GSE32063_eset)[\"ZNF487\",])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nconcordance.formula(object = SurvObj ~ exprs(GSE32063_eset)[\"ZNF487\", \n    ])\n\nn= 40 \nConcordance= 0.6389 se= 0.05912\nconcordant discordant     tied.x     tied.y    tied.xy \n       253        143          0          8          0 \n```\n\n\n:::\n:::\n\n\nThe C-index was introduced to measure the discrimination of a risk prediction model, in which case the prediction is a real or ordinal valued risk score, and the model works well if low risk goes with longer survival. \n\n\n\\newpage\n\n### Log Hazard Modeling\n\nThe next level of analysis is regression.\nThe most common way to model time-to-event data, with or without censoring, is via hazard (or intensity) functions. \n\n[@KraghAndersen2021sim](https://doi-org.ezp-prod1.hul.harvard.edu/10.1002/sim.8757) provide a thoughtful and concise introduction to time-to-event data. Highly recommended.\n\n### Exponential and Weibull Time-to-Event Distributions \n\nLet's start our discussion by calculating the hazard function for Exponential and  Weibull distribution.\n\nThe Weibull fits a lot of cancer survival data fairly well. [@Plana2022nc]({https://doi.org/10.1038/s41467-022-28410-9) looks at Weibull fit across hundreds of studies ---very interesting work.\n\n\\begin{Large}\n\\begin{eqnarray*}\n\\mbox{Exponential Hazard -- Constant in time:} \\;\\;\\;\\; h(t) & = & \\lambda \\\\\n\\mbox{Weibull Hazard -- Polynomial in time} \\;\\;\\;\\; \nh(t) & = & v \\lambda t ^{v-1} \\\\\n\\mbox{Weibull Survival:} \\;\\;\\;\\; S(t) & =&  e^{-\\lambda t ^v}\n\\end{eqnarray*}\n\\end{Large}\n\nThe hazard of the weibull is a polynomial function of time. The $v$ parameter controls whether the event rate is slowing down or picking up speed with time. $\\lambda$ acts as an intercept for hazard. A popular tactic is to model $log h$, via $\\log \\lambda$ as a function of the biomarker. \n\n\\begin{Large}\n\\begin{eqnarray*}\n\\mbox{Weibull Log Hazard:} \\;\\;\\;\\; \\log h(t) & = & log (v) + (v-1) log (t) + log \\lambda \\\\\n\\mbox{With Biomarker:} \\;\\;\\;\\; \\log h_x(t)  & = & [ log (v) + (v-1) log(t) ] + \\beta x \\\\\n& = & \\mbox{[ Baseline ] + Biomarker Effect} \\\\\n\\mbox{Cox Proportional Hazard:} \\;\\;\\;\\; \\log h_x(t) & = & \\log h_0(t) + \\beta x \\\\\n\\end{eqnarray*}\n\\end{Large}\n\nTo illustrate this, we will consider the `POSTN` gene within the `GSE32063_eset` dataset. \nIn the coding chunk that follows, we will consider `XX` as the gene expression and `XXgmedian` as the binary variable created through dichotomizing the expression values at the median.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(curatedOvarianData)\ndata(GSE32063_eset)\nlibrary(survival)\nGeneName = \"POSTN\"\nXX = exprs(GSE32063_eset)[GeneName,]\nXXgmedian = 1 * ( XX > median(XX) )\nSurvObj = \n  Surv( time = pData(GSE32063_eset)[,\"days_to_death\"], \n        event = pData(GSE32063_eset)[,\"vital_status\"]==\"deceased\")\nPOSTN.frame = data.frame(time = pData(GSE32063_eset)[,\"days_to_death\"], \n              event = pData(GSE32063_eset)[,\"vital_status\"]==\"deceased\",\n              expression = XX,\n              gtmedian = XXgmedian)\n```\n:::\n\nThe next yields the scatterplot and the Kaplan-Meier curves stratified by an indicator of whether the biomarker exceeds the population median (with and w/out confidence bands), for illustration. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npendant.plot(time=pData(GSE32063_eset)[,\"days_to_death\"],\n          status=pData(GSE32063_eset)[,\"vital_status\"],\n          biomarker=exprs(GSE32063_eset)[\"POSTN\",])\nkm.by.gene = survfit (SurvObj ~ XXgmedian)\nggsurvplot(km.by.gene,data=POSTN.frame)\n```\n\n::: {.cell-output-display}\n![Kaplan-Meier Curve on Median Cut-off](L8-timetoevent_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![Kaplan-Meier Curve on Median Cut-off](L8-timetoevent_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggsurvplot(km.by.gene,data=POSTN.frame,conf.int=TRUE)\n```\n\n::: {.cell-output-display}\n![Kaplan-Meier Curve on Median Cut-off](L8-timetoevent_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n:::\n\n\n\\newpage\nThe following gives the survival concordance objects. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nconcordance(SurvObj ~ XXgmedian, reverse=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nconcordance.formula(object = SurvObj ~ XXgmedian, reverse = TRUE)\n\nn= 40 \nConcordance= 0.5821 se= 0.06193\nconcordant discordant     tied.x     tied.y    tied.xy \n       137         72        187          4          4 \n```\n\n\n:::\n\n```{.r .cell-code}\nconcordance(SurvObj ~ XX, reverse=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nconcordance.formula(object = SurvObj ~ XX, reverse = TRUE)\n\nn= 40 \nConcordance= 0.6389 se= 0.07398\nconcordant discordant     tied.x     tied.y    tied.xy \n       253        143          0          8          0 \n```\n\n\n:::\n:::\n\n\nThe loss of concordance gives you one way to think about the information loss from dichotimization.\n\n\\newpage\n\nThen, we can call out the Cox-Proportional Hazards Model for both the median cut-off variable and the entire gene expression variable.\nThis is a concise introduction to assumptions and interpretation in\n[Cox Proportional Hazards Regression Analysis](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/BS704_Survival6.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(coxph(SurvObj~XXgmedian))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\ncoxph(formula = SurvObj ~ XXgmedian)\n\n  n= 40, number of events= 22 \n\n            coef exp(coef) se(coef)     z Pr(>|z|)  \nXXgmedian 0.7955    2.2156   0.4544 1.751     0.08 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n          exp(coef) exp(-coef) lower .95 upper .95\nXXgmedian     2.216     0.4513    0.9092     5.399\n\nConcordance= 0.582  (se = 0.062 )\nLikelihood ratio test= 3.18  on 1 df,   p=0.07\nWald test            = 3.06  on 1 df,   p=0.08\nScore (logrank) test = 3.22  on 1 df,   p=0.07\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(coxph(SurvObj~XX))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\ncoxph(formula = SurvObj ~ XX)\n\n  n= 40, number of events= 22 \n\n      coef exp(coef) se(coef)     z Pr(>|z|)  \nXX 0.15808   1.17126  0.08006 1.975   0.0483 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n   exp(coef) exp(-coef) lower .95 upper .95\nXX     1.171     0.8538     1.001      1.37\n\nConcordance= 0.639  (se = 0.074 )\nLikelihood ratio test= 3.94  on 1 df,   p=0.05\nWald test            = 3.9  on 1 df,   p=0.05\nScore (logrank) test = 4.1  on 1 df,   p=0.04\n```\n\n\n:::\n:::\n\n\n::: callout-note\n### Metrics\n* Does this regression provide useful metrics for evaluating whether POSTN is a useful biomarker?\n\n* What are some of their strengths and limitations?\n:::\n\n\\newpage\n\n\nNow, we can also fit the Cox Proportional Hazards Model to POSTN, which is now a full continuous biomarker, and plot the predicted survival curve for an individual with a biomarker value of $5$ for POSTN.\n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- coxph(SurvObj ~ XX)\npar(pty = \"s\", mfrow = c(1,1))\nplot(survfit(fit, newdata=data.frame(XX = - 5)), xscale=365.25, xlab = \"Years\", ylab=\"Survival Probability\", col = c(1,3,3))\n```\n\n::: {.cell-output-display}\n![Cox Proportional Hazards Curve for Individual with POSTN Level of 5](L8-timetoevent_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n::: callout-note\n### $Y|X$ versus $X|Y$\n\nWith binary data, we operated by modeling class conditional distributions $X|Y$, and converting to PPV using Bayes rule. With survival data we modeled $Y|X$. \n\n* What are some of the pros and cons of the two approaches?\n\n* What is the equivalent to PPV in time-to-event data?\n:::\n\n\\newpage\n## References",
    "supporting": [
      "L8-timetoevent_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}